{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4752dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import io\n",
    "from skimage.measure import regionprops, label\n",
    "from skimage.color import rgb2gray\n",
    "import mahotas as mh\n",
    "from skimage.feature import graycomatrix, graycoprops, local_binary_pattern\n",
    "import networkx as nx\n",
    "from scipy.stats import pearsonr\n",
    "import torch\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool, global_max_pool, BatchNorm\n",
    "from torch_geometric.data import DataLoader\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Função para carregar o JSON\n",
    "def load_json(json_path):\n",
    "    with open(json_path) as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "# Função para obter o bounding box ao redor do núcleo\n",
    "def get_bounding_box(nucleus_x, nucleus_y, box_size=100):\n",
    "    half_size = box_size // 2\n",
    "    min_row = max(nucleus_y - half_size, 0)\n",
    "    max_row = min(nucleus_y + half_size, 1020)\n",
    "    min_col = max(nucleus_x - half_size, 0)\n",
    "    max_col = min(nucleus_x + half_size, 1376)\n",
    "    return (min_row, max_row, min_col, max_col)\n",
    "\n",
    "# Função para extrair regiões das imagens\n",
    "def extract_regions(image, json_data, box_size=100):\n",
    "    regions = []\n",
    "    for cell in json_data:\n",
    "        nucleus_x = cell['nucleus_x']\n",
    "        nucleus_y = cell['nucleus_y']\n",
    "        \n",
    "        min_row, max_row, min_col, max_col = get_bounding_box(nucleus_x, nucleus_y, box_size)\n",
    "        region = image[min_row:max_row, min_col:max_col]\n",
    "        regions.append(region)\n",
    "    \n",
    "    return regions\n",
    "\n",
    "# Função para extrair atributos de uma região\n",
    "def extract_features_from_region(region):\n",
    "    gray_region = rgb2gray(region)\n",
    "    \n",
    "    if np.sum(gray_region) == 0:\n",
    "        print(\"Erro: Região em branco ou inválida.\")\n",
    "        return None\n",
    "\n",
    "    gray_region = (gray_region * 255).astype(np.uint8)\n",
    "    labeled_region = label(gray_region)\n",
    "\n",
    "    props = regionprops(labeled_region)\n",
    "    if len(props) == 0:\n",
    "        return None\n",
    "\n",
    "    props = props[0]\n",
    "\n",
    "    # Características básicas\n",
    "    area = props.area\n",
    "    perimeter = props.perimeter\n",
    "    eccentricity = props.eccentricity\n",
    "    solidity = props.solidity\n",
    "    equivalent_diameter = props.equivalent_diameter\n",
    "    convex_area = props.convex_area\n",
    "    extent = props.extent\n",
    "    major_axis_length = props.major_axis_length\n",
    "    minor_axis_length = props.minor_axis_length\n",
    "\n",
    "    # Características de intensidade\n",
    "    mean_intensity = np.mean(gray_region)\n",
    "    std_intensity = np.std(gray_region)\n",
    "\n",
    "    # Características de textura usando GLCM\n",
    "    glcm = graycomatrix(gray_region, distances=[1], angles=[0], levels=256, symmetric=True, normed=True)\n",
    "    glcm_entropy = -np.sum(glcm * np.log(glcm + np.finfo(float).eps))\n",
    "    contrast = graycoprops(glcm, 'contrast')[0, 0]\n",
    "\n",
    "    # Extraindo características de Haralick\n",
    "    haralick_features = mh.features.haralick(gray_region).mean(axis=0)\n",
    "\n",
    "    # Extraindo características de LBP\n",
    "    lbp = local_binary_pattern(gray_region, P=8, R=1, method='uniform')\n",
    "    lbp_hist, _ = np.histogram(lbp, bins=np.arange(0, 11), range=(0, 10))\n",
    "\n",
    "    # Novas características estatísticas\n",
    "    skewness = np.mean((gray_region - mean_intensity)**3) / (std_intensity**3) if std_intensity > 0 else 0\n",
    "    kurtosis = np.mean((gray_region - mean_intensity)**4) / (std_intensity**4) - 3 if std_intensity > 0 else 0\n",
    "\n",
    "    # Preenchendo a lista de características\n",
    "    features = [\n",
    "        area, perimeter, eccentricity, solidity, equivalent_diameter, convex_area,\n",
    "        extent, major_axis_length, minor_axis_length, mean_intensity,\n",
    "        std_intensity, glcm_entropy, contrast, skewness, kurtosis\n",
    "    ] + list(haralick_features) + list(lbp_hist)\n",
    "\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Função para mapear classificações para rótulos\n",
    "def map_labels(classifications):\n",
    "    labels = []\n",
    "    for classification_group in classifications:\n",
    "        for cell in classification_group:\n",
    "            if cell['bethesda_system'] == 'POSITIVE':\n",
    "                labels.append(1)\n",
    "            else:\n",
    "                labels.append(0)\n",
    "    return torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "# Função para verificar se os grafos criados contêm nós e arestas válidos\n",
    "def build_graph_across_cells(features, threshold=0.8):\n",
    "    num_cells = len(features)\n",
    "    num_attributes = len(features[0])  # Número de atributos por célula\n",
    "    \n",
    "    graphs = []\n",
    "    \n",
    "    # Agora vamos construir o grafo para cada célula\n",
    "    for feature_set in features:\n",
    "        G = nx.Graph()\n",
    "        \n",
    "        # Adiciona os nós (atributos) com cada feature como um nó individual\n",
    "        for i, feature in enumerate(feature_set):\n",
    "            G.add_node(i, feature=feature)\n",
    "        \n",
    "        # Calcula a correlação de Pearson entre os atributos de diferentes células\n",
    "        for i in range(num_attributes):\n",
    "            for j in range(i + 1, num_attributes):\n",
    "                # Calcula a correlação entre os atributos i e j em várias células\n",
    "                attribute_i = [cell[i] for cell in features]\n",
    "                attribute_j = [cell[j] for cell in features]\n",
    "                \n",
    "                # Verifica se ambos os arrays têm variância para evitar cálculo em arrays constantes\n",
    "                if np.std(attribute_i) > 0 and np.std(attribute_j) > 0:\n",
    "                    corr, _ = pearsonr(attribute_i, attribute_j)\n",
    "                    \n",
    "                    # Se a correlação for maior que o threshold, cria uma aresta\n",
    "                    if corr > threshold:\n",
    "                        G.add_edge(i, j, weight=corr)\n",
    "        \n",
    "        # Verificar se o grafo contém nós e arestas válidos\n",
    "        if G.number_of_nodes() > 0 and G.number_of_edges() > 0:\n",
    "            graphs.append(G)\n",
    "    \n",
    "    return graphs\n",
    "\n",
    "def create_pyg_data_with_batching(features, graphs, labels):\n",
    "    data_list = []\n",
    "    for i in range(len(features)):\n",
    "        num_features_por_nó = len(features[i]) if features[i] is not None else 0\n",
    "        \n",
    "        if num_features_por_nó == 0:\n",
    "            print(f\"Erro: Não há features para o índice {i}.\")\n",
    "            continue\n",
    "\n",
    "        x = torch.tensor(features[i], dtype=torch.float).view(-1, num_features_por_nó)\n",
    "\n",
    "        # Verifique se graphs[i] é realmente um grafo com o método edges\n",
    "        if not hasattr(graphs[i], 'edges'):\n",
    "            print(f\"Erro: O item graphs[{i}] não é um grafo válido. Tipo encontrado: {type(graphs[i])}\")\n",
    "            continue\n",
    "\n",
    "        edge_index = torch.tensor(list(graphs[i].edges), dtype=torch.long).t().contiguous()\n",
    "        \n",
    "        if edge_index.size(0) != 2:\n",
    "            print(f\"Erro: edge_index para o índice {i} não tem formato correto.\")\n",
    "            continue\n",
    "        \n",
    "        if edge_index.max().item() >= x.size(0):\n",
    "            print(f\"Erro: edge_index para o índice {i} tem valores fora do intervalo. Max index: {edge_index.max().item()} para num_nodes: {x.size(0)}\")\n",
    "            continue\n",
    "\n",
    "        label = labels[i] if i < len(labels) else -1\n",
    "        data = Data(x=x, edge_index=edge_index, y=torch.tensor([label], dtype=torch.long))\n",
    "        \n",
    "        print(f\"Data {i} - x shape: {data.x.shape}, edge_index shape: {data.edge_index.shape}, y: {data.y}\")\n",
    "        data_list.append(data)\n",
    "\n",
    "    return data_list\n",
    "\n",
    "\n",
    "# Definição do modelo GCN com Global Pooling\n",
    "class GCNGraphLevel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, max_features=136, dropout=0.5):\n",
    "        super(GCNGraphLevel, self).__init__()\n",
    "        self.max_features = max_features\n",
    "        self.fc = nn.Linear(max_features, max_features)  # Ajusta a entrada para max_features\n",
    "        self.batch_norm1 = BatchNorm(max_features)\n",
    "        self.conv1 = GCNConv(max_features, hidden_dim)\n",
    "        self.batch_norm2 = BatchNorm(hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
    "        self.batch_norm3 = BatchNorm(hidden_dim)\n",
    "        self.conv3 = GCNConv(hidden_dim, output_dim)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, data):\n",
    "        # Padding das features\n",
    "        x = data.x\n",
    "        if x.size(1) < self.max_features:\n",
    "            padding = torch.zeros(x.size(0), self.max_features - x.size(1), device=x.device)\n",
    "            x = torch.cat([x, padding], dim=1)\n",
    "        elif x.size(1) > self.max_features:\n",
    "            x = x[:, :self.max_features]\n",
    "\n",
    "        # Camada densa e normalização\n",
    "        x = self.fc(x)\n",
    "        x = self.batch_norm1(x)\n",
    "        edge_index = data.edge_index\n",
    "        \n",
    "        # Normalização do grafo\n",
    "        num_nodes = x.size(0)\n",
    "        edge_index, edge_weight = gcn_norm(edge_index, data.edge_attr, num_nodes=num_nodes, dtype=x.dtype)\n",
    "        \n",
    "        # Camadas convolucionais GCN com normalização e dropout\n",
    "        x = self.conv1(x, edge_index, edge_weight)\n",
    "        x = self.batch_norm2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = self.batch_norm3(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "\n",
    "        x = self.conv3(x, edge_index)\n",
    "        \n",
    "        # Pooling global\n",
    "        x = global_mean_pool(x, data.batch)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "# Função de treino que inclui validação\n",
    "def train_with_validation(model, train_loader, val_loader, optimizer, criterion, epochs=1000, log_interval=10):\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    val_accuracies = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0  # Acumular a perda por época\n",
    "        for batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            out = model(batch)\n",
    "            loss = criterion(out, batch.y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        # Armazena a perda média a cada log_interval épocas\n",
    "        if (epoch + 1) % log_interval == 0:\n",
    "            avg_loss = epoch_loss / len(train_loader)\n",
    "            train_losses.append(avg_loss)\n",
    "\n",
    "            val_accuracy = test(model, val_loader)\n",
    "            val_accuracies.append(val_accuracy)\n",
    "            print(f'Epoch {epoch + 1}, Avg Loss: {avg_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}')\n",
    "\n",
    "    return train_losses, val_accuracies\n",
    "\n",
    "# Função para testar o modelo\n",
    "def test(model, loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch in loader:  # Iterar sobre os batches no DataLoader\n",
    "        out = model(batch)\n",
    "        pred = out.argmax(dim=1)\n",
    "        correct += (pred == batch.y).sum().item()\n",
    "        total += len(batch.y)\n",
    "    return correct / total\n",
    "\n",
    "\n",
    "# Diretórios das imagens e arquivos JSON\n",
    "train_image_dir = \"/Users/xr4good/Documents/Ingrid/datasets/imagens/treino\"\n",
    "train_json_path = \"/Users/xr4good/Documents/Ingrid/datasets/imagens/treino/treino.json\"\n",
    "val_image_dir = \"/Users/xr4good/Documents/Ingrid/datasets/imagens/validacao\"\n",
    "val_json_path = \"/Users/xr4good/Documents/Ingrid/datasets/imagens/validacao/validacao.json\"\n",
    "test_image_dir = \"/Users/xr4good/Documents/Ingrid/datasets/imagens/testeboundingbox\"\n",
    "test_json_path = \"/Users/xr4good/Documents/Ingrid/datasets/imagens/testeboundingbox/teste.json\"\n",
    "\n",
    "# Especificar o diretório de saída\n",
    "output_dir = \"/Users/xr4good/Documents/Ingrid/\"\n",
    "\n",
    "\n",
    "# Carregar JSONs\n",
    "train_data = load_json(train_json_path)\n",
    "val_data = load_json(val_json_path)\n",
    "test_data = load_json(test_json_path)\n",
    "\n",
    "\n",
    "# Função para processar um conjunto de dados e construir os grafos e features\n",
    "def process_data(data, image_dir, image_range):\n",
    "    features_list = []\n",
    "    graphs_list = []\n",
    "    labels_list = []\n",
    "\n",
    "    for entry in data:\n",
    "        image_id = entry['image_id']\n",
    "        \n",
    "        # Verificar se a imagem está dentro do intervalo desejado\n",
    "        if image_id not in image_range:\n",
    "            continue\n",
    "        \n",
    "        image_name = f\"cric_{image_id}.png\"\n",
    "        image_path = os.path.join(image_dir, image_name)\n",
    "        \n",
    "        image = io.imread(image_path)\n",
    "        if image.shape[2] == 4:  # Converter para RGB se necessário\n",
    "            image = image[:, :, :3]\n",
    "\n",
    "        classifications = entry['classifications']\n",
    "        \n",
    "        # Extrair regiões e construir grafo para cada célula na imagem\n",
    "        features = []\n",
    "        labels = []\n",
    "        regions = extract_regions(image, classifications)\n",
    "        \n",
    "        for idx, region in enumerate(regions):\n",
    "            feature_vector = extract_features_from_region(region)\n",
    "            if feature_vector is not None:\n",
    "                features.append(feature_vector)\n",
    "                bethesda_system = classifications[idx]['bethesda_system']\n",
    "                labels.append(1 if bethesda_system == \"POSITIVE\" else 0)\n",
    "        \n",
    "        # Construir o grafo com base nas features\n",
    "        if features:\n",
    "            cell_graphs = build_graph_across_cells(features)\n",
    "            features_list.append(features)\n",
    "            graphs_list.extend(cell_graphs)  # Adicionar cada grafo individualmente\n",
    "            labels_list.extend(labels)  # Adicionar todas as labels das células\n",
    "\n",
    "    return features_list, graphs_list, labels_list\n",
    "\n",
    "# Carregar JSONs\n",
    "train_data = load_json(train_json_path)\n",
    "val_data = load_json(val_json_path)\n",
    "test_data = load_json(test_json_path)\n",
    "\n",
    "# Definir intervalos de imagens para cada conjunto\n",
    "train_image_range = list(range(1, 160)) + list(range(162, 281))  # cric_1 a cric_280, exceto 160 e 161\n",
    "val_image_range = list(range(282, 343))  # cric_282 a cric_342\n",
    "test_image_range = list(range(343, 400))  # cric_343 a cric_399\n",
    "\n",
    "# Processar os conjuntos de treino, validação e teste\n",
    "train_features, train_graphs, train_labels = process_data(train_data, train_image_dir, train_image_range)\n",
    "val_features, val_graphs, val_labels = process_data(val_data, val_image_dir, val_image_range)\n",
    "test_features, test_graphs, test_labels = process_data(test_data, test_image_dir, test_image_range)\n",
    "\n",
    "# Preparar dados para PyTorch Geometric\n",
    "train_data_list = create_pyg_data_with_batching(train_features, train_graphs, train_labels)\n",
    "val_data_list = create_pyg_data_with_batching(val_features, val_graphs, val_labels)\n",
    "test_data_list = create_pyg_data_with_batching(test_features, test_graphs, test_labels)\n",
    "\n",
    "# Criar DataLoaders\n",
    "train_loader = DataLoader(train_data_list, batch_size=1, shuffle=True)\n",
    "val_loader = DataLoader(val_data_list, batch_size=1, shuffle=False)\n",
    "test_loader = DataLoader(test_data_list, batch_size=1, shuffle=False)\n",
    "\n",
    "# Definir o modelo, critério e otimizador\n",
    "num_node_features = len(train_features[0][0])  # Número de características de cada nó\n",
    "num_classes = 2  # Classificação binária\n",
    "model = GCNGraphLevel(num_node_features, hidden_dim=64, output_dim=num_classes)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# Definindo o otimizador com weight decay\n",
    "learning_rate =  0.00005  # Ajuste de acordo com sua necessidade\n",
    "weight_decay = 1e-3  # Por exemplo, 0.0001\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "\n",
    "# Treinar o modelo com validação\n",
    "epochs = 1000\n",
    "# Treinar o modelo\n",
    "train_losses, val_accuracies = train_with_validation(\n",
    "    model, train_loader, val_loader, optimizer, criterion, epochs=1000, log_interval=10\n",
    ")\n",
    "\n",
    "\n",
    "# Plotar os resultados\n",
    "epochs_range = list(range(10, epochs + 1, 10))  # Intervalos de 10 épocas\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, train_losses, label='Train Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Train Loss over Epochs')\n",
    "plt.legend()\n",
    "\n",
    "# Salvar o gráfico de Train Loss\n",
    "train_loss_path = os.path.join(output_dir, \"train_loss.png\")\n",
    "plt.savefig(train_loss_path)\n",
    "print(f\"Gráfico de Train Loss salvo em {train_loss_path}\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, val_accuracies, label='Validation Accuracy')\n",
    "plt.xlabel('Epoch (every 10)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Validation Accuracy over Epochs')\n",
    "plt.legend()\n",
    "\n",
    "# Salvar o gráfico de Validation Accuracy\n",
    "val_accuracy_path = os.path.join(output_dir, \"val_accuracy.png\")\n",
    "plt.savefig(val_accuracy_path)\n",
    "print(f\"Gráfico de Validation Accuracy salvo em {val_accuracy_path}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Testar o modelo no conjunto de teste\n",
    "test_accuracy = test(model, test_loader)\n",
    "print(f'Test Accuracy: {test_accuracy:.4f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (RedesComplexas)",
   "language": "python",
   "name": "redescomplexas"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
